{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS6910_Assignment2A.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwqupWe2EiX0"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "id": "F4KS_WtpFINb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Activation\n"
      ],
      "metadata": {
        "id": "zMzragLCFdrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X4DPUehUFm-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/Dataset /nature_12K.zip\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q nature_12K.zip\n",
        "!rm nature_12K.zip"
      ],
      "metadata": {
        "id": "d_NmlzlnFzZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the wandb library\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "gDMakTjuF34v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "DP-vav_OF_2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the training and testing directories\n",
        "import os \n",
        "Tags = ['Amphibia','Animalia','Arachnida','Aves','Fungi','Insecta','Mammalia','Mollusca','Plantae','Reptilia']\n",
        "no_class = 10\n",
        "train_dir='inaturalist_12K/train/'\n",
        "test_dir='inaturalist_12K/val/'"
      ],
      "metadata": {
        "id": "LD323r8vGHTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing images of each class \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "import matplotlib.image as mpimg\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "i=1\n",
        "for tag in Tags:\n",
        "  dir=os.path.join(train_dir,tag)\n",
        "  image_name=os.listdir(dir)[0]\n",
        "  image_path=os.path.join(dir,image_name)\n",
        "  img=mpimg.imread(image_path)\n",
        "  img_req=cv2.resize(img,(128,128)) \n",
        "  fig.add_subplot(2,5,i)\n",
        "  plt.imshow(img_req)\n",
        "  plt.axis('off')\n",
        "  plt.title(tag)\n",
        "  i+=1   \n"
      ],
      "metadata": {
        "id": "SKQgrRkLGIVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for training\n",
        "def train_val__test_data_generation(batch_size=256,augment_data=False):\n",
        "\n",
        "    if augment_data:\n",
        "        train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                          rotation_range=45,\n",
        "                                          zoom_range=0.2,\n",
        "                                          shear_range=0.2,\n",
        "                                          validation_split=0.1,\n",
        "                                          horizontal_flip=True,\n",
        "                                          vertical_flip=False)\n",
        "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    else:\n",
        "        train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
        "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(train_dir, target_size=(128,128), batch_size=batch_size, subset=\"training\")\n",
        "    val_generator = train_datagen.flow_from_directory(train_dir, target_size=(128,128), batch_size=batch_size, subset=\"validation\")\n",
        "    test_generator = test_datagen.flow_from_directory(test_dir, target_size=(128,128), batch_size=batch_size)\n",
        "    \n",
        "    return train_generator, val_generator, test_generator"
      ],
      "metadata": {
        "id": "JZunh5IDGLJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, val_generator, test_generator = train_val__test_data_generation(batch_size=64,augment_data=True)"
      ],
      "metadata": {
        "id": "DD7jQLq9GSKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a small CNN model consisting of 5 convolution layers.\n",
        "\n",
        "def CNN(filters,filter_size, image_size=128,\n",
        "              dropout=0.2,batch_norm=False, dense_size=64, \n",
        "              regpara=0, no_of_classes=10, activation='relu'):\n",
        "\n",
        "    model = Sequential()\n",
        "    for i in range(5):\n",
        "        if(i==0):\n",
        "            model.add(Conv2D(filters=filters[i], kernel_size=filter_size[i], padding = 'same', input_shape = (image_size, image_size, 3),\n",
        "                             kernel_regularizer= regularizers.l2(regpara)))\n",
        "        else:\n",
        "            model.add(Conv2D(filters=filters[i], kernel_size=filter_size[i], padding = 'same',\n",
        "                             kernel_regularizer= regularizers.l2(regpara)))\n",
        "        \n",
        "        model.add(Activation(activation))\n",
        "\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    \n",
        "    # FC layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(dense_size, activation=\"relu\",kernel_regularizer= regularizers.l2(regpara)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    #Output Layer\n",
        "    model.add(Dense(no_of_classes, activation = \"softmax\"))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "s2PUBHzFGtTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'kernel_size':{\n",
        "            'values': [[(2,2),(2,2),(2,2),(2,2),(2,2)], [(3,3),(3,3),(3,3),(3,3),(3,3)],[(6,6),(5,5),(4,4),(3,3),(2,2)],[(2,2),(3,3),(4,4),(5,5),(6,6)]]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.005]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.3, 0.4]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu','selu','elu']\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': [True,False]\n",
        "        },\n",
        "        'filters':{\n",
        "            'values': [[32,32,32,32,32],[32,64,128,256,512],[32,16,8,4,2],[512,256,128,64,32]]\n",
        "        },\n",
        "        'augment_data': {\n",
        "            'values': [True,False]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'dense_size':{\n",
        "            'values': [64, 128, 256, 512]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "WRGyqmhzGxXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    \n",
        "    config_defaults = {\n",
        "        'kernel_size': [(3,3),(3,3),(3,3),(3,3),(3,3)],\n",
        "        'weight_decay': 0.005,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 1e-3,\n",
        "        'activation': 'relu',\n",
        "        'batch_size': 64,\n",
        "        'epochs': 10,\n",
        "        'batch_norm': True,\n",
        "        'filters' : [32,32,32,32,32],\n",
        "        'augment_data': True,\n",
        "        'dense_size': 256,\n",
        "        'seed': 1234,\n",
        "        'no_of_classes': 10\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "\n",
        "    config = wandb.config\n",
        "    wandb.run.name = 'dense_size_'+ str(config.dense_size)+'_bs_'+str(config.batch_size)+'_ac_'+ config.activation\n",
        "\n",
        "    #tf.keras.backend.clear_session()\n",
        "\n",
        "    model=CNN(filters=config.filters,filter_size=config.kernel_size, image_size=128,\n",
        "              dropout=config.dropout,batch_norm=config.batch_norm, dense_size=config.dense_size, \n",
        "              regpara=config.weight_decay, no_of_classes=config.no_of_classes, activation=config.activation )\n",
        "    \n",
        "    optimizer = Adam(learning_rate=config.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "    #early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3,restore_best_weights=True)\n",
        "\n",
        "\n",
        "    train_generator, val_generator, test_generator = train_val__test_data_generation(batch_size=config.batch_size,augment_data=config.augment_data)\n",
        "\n",
        "    hist=model.fit(train_generator, epochs=config.epochs, validation_data=val_generator, callbacks=[WandbCallback()])\n",
        "    val_acc=max(hist.history['val_accuracy'])\n",
        "    params={'batch_norm':config.batch_norm,'augmentation':config.augment_data,'dropout':config.dropout,\n",
        "            'filter_architecture':config.filters,'kernel_size':config.kernel_size,'val_acc':val_acc}\n",
        "    wandb.log(params)"
      ],
      "metadata": {
        "id": "2bUL853wG4oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_DL_Assignment2\", entity=\"nomads\")"
      ],
      "metadata": {
        "id": "gLT2_q8LG_-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent('twsg746e', train, count = 50)"
      ],
      "metadata": {
        "id": "_th4GZNDHWMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GHF0X9WfInSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IqHng0HpIpF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}